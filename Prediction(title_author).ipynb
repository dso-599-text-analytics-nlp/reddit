{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"reddit_worldnews_start_to_2016-11-22.csv\", encoding='latin-1')\n",
    "data = data.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>date_created</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>title</th>\n",
       "      <th>over_18</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>285199</th>\n",
       "      <td>1403789552</td>\n",
       "      <td>2014-06-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pakistanis fleeing North Waziristan demand ans...</td>\n",
       "      <td>False</td>\n",
       "      <td>r4816</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86332</th>\n",
       "      <td>1304917313</td>\n",
       "      <td>2011-05-09</td>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>Ecuador bans 500-yr-old tradition of bullfight...</td>\n",
       "      <td>False</td>\n",
       "      <td>maxwellhill</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131197</th>\n",
       "      <td>1344633757</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Officials need help finding missing Olympians</td>\n",
       "      <td>False</td>\n",
       "      <td>CarlSagginPants</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389415</th>\n",
       "      <td>1439551371</td>\n",
       "      <td>2015-08-14</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>Some Isolated Tribes in the Amazon Are Initiat...</td>\n",
       "      <td>False</td>\n",
       "      <td>anutensil</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113067</th>\n",
       "      <td>1328187791</td>\n",
       "      <td>2012-02-02</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>London Metropolitan Police  inadvertently  sha...</td>\n",
       "      <td>False</td>\n",
       "      <td>anutensil</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_created date_created  up_votes  down_votes  \\\n",
       "285199    1403789552   2014-06-26         0           0   \n",
       "86332     1304917313   2011-05-09       850           0   \n",
       "131197    1344633757   2012-08-10         2           0   \n",
       "389415    1439551371   2015-08-14        35           0   \n",
       "113067    1328187791   2012-02-02        17           0   \n",
       "\n",
       "                                                    title  over_18  \\\n",
       "285199  Pakistanis fleeing North Waziristan demand ans...    False   \n",
       "86332   Ecuador bans 500-yr-old tradition of bullfight...    False   \n",
       "131197      Officials need help finding missing Olympians    False   \n",
       "389415  Some Isolated Tribes in the Amazon Are Initiat...    False   \n",
       "113067  London Metropolitan Police  inadvertently  sha...    False   \n",
       "\n",
       "                 author  subreddit  \n",
       "285199            r4816  worldnews  \n",
       "86332       maxwellhill  worldnews  \n",
       "131197  CarlSagginPants  worldnews  \n",
       "389415        anutensil  worldnews  \n",
       "113067        anutensil  worldnews  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['year']='2008'\n",
    "for i in data.index:\n",
    "    data.at[i,'year'] = data.at[i,'date_created'][:4]\n",
    "    \n",
    "# temp is a dict that contains the 95% cutoff of each year\n",
    "temp={}\n",
    "for i in range(2008,2017):\n",
    "    temp[str(i)] = np.percentile(data[data['year']==str(i)]['up_votes'],95)\n",
    "\n",
    "data['label']=0\n",
    "for i in data.index:\n",
    "    if data.at[i,'up_votes'] >= temp[data.at[i,'year']]:\n",
    "        data.at[i,'label']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in spacy\n",
    "import en_core_web_md\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "# Preprocess the reviews (tokenizing, lemmatization, removing stopwords)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(titles):\n",
    "    filtered_titles = []\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        token_list = word_tokenize(title) # Tokenize\n",
    "        filtered_token = [t for t in token_list if not t in stop_words] # Remove stopwords\n",
    "        for i in range(len(filtered_token)):\n",
    "            filtered_token[i] = lemmatizer.lemmatize(filtered_token[i]).strip(string.punctuation) # Lemmatization\n",
    "        filtered_titles.append(\" \".join(filtered_token))\n",
    "    return filtered_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Weighted Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF vectorizer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# data\n",
    "filtered_corpus = preprocessing(data[\"title\"])\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df = 0.4, \n",
    "                             max_features = 2000) # only use first 2000 features because of \n",
    "                                                  # computatioal complexity later on\n",
    "\n",
    "# vectorize the corpus\n",
    "vector = vectorizer.fit_transform(filtered_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF matrix\n",
    "tfidf_matrix = pd.DataFrame(vector.toarray(), columns = vectorizer.get_feature_names(), index = data.index)\n",
    "# Word embeddings for each word in the column index of TF-IDF matrix\n",
    "word2vec = [np.array(nlp(i).vector) for i in tfidf_matrix.columns]\n",
    "# For each title, use each word's TF-IDF mutliply by its word embeddings vector and sum all the word vectors\n",
    "# The result is an unweighted matrix for each title\n",
    "unweighted_matrix = pd.DataFrame(np.dot(tfidf_matrix,np.array(word2vec)), index = tfidf_matrix.index)\n",
    "#unweighted_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each title, use unweighted matrix divided by the sum of that title's TF-IDF to get weighted word2vec matrix\n",
    "# The result is our final word2vec matrix\n",
    "final_w2v = unweighted_matrix.div(tfidf_matrix.sum(axis=1), axis=0)\n",
    "final_w2v = final_w2v.fillna(0)\n",
    "#final_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_w2v[\"label\"] = np.array(data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reddit_train, reddit_test, y_train, y_test = train_test_split(\n",
    "    final_w2v,\n",
    "    final_w2v[\"label\"],\n",
    "    test_size=0.3,\n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Variables -- Author_Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reddit_train = reddit_train.drop(columns = [\"label\"])\n",
    "reddit_test = reddit_test.drop(columns = [\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = data.loc[list(reddit_train.index),:]\n",
    "data_test = data.loc[list(reddit_test.index),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_author_y = data_train[[\"author\", \"label\"]]\n",
    "author_mean = data_author_y.groupby(by = \"author\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train['author_value'] = data_train['author'].map(author_mean['label'])\n",
    "author_value_mean = data_train['author_value'].mean()\n",
    "data_test['author_value'] = data_test['author'].map(author_mean['label'])\n",
    "data_test = data_test.fillna(author_value_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_train[\"author_value\"] = np.array(data_train[\"author_value\"])\n",
    "reddit_test[\"author_value\"] = np.array(data_test[\"author_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_variables = pd.DataFrame(np.concatenate((reddit_train,reddit_test)), columns = reddit_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_row = len(reddit_train)\n",
    "test_row = len(reddit_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# keep 90% of original information\n",
    "pca = PCA(n_components = 0.8)\n",
    "pca_features = pca.fit_transform(np.array(final_variables))\n",
    "pca_df = pd.DataFrame(pca_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 85)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pca_df[:int(train_row)]\n",
    "X_test = pca_df[-int(test_row):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "re = LogisticRegression()\n",
    "re.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = re.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9808571428571429"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9433333333333334"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5156154456927472"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_prob=re.predict_proba(X_train)\n",
    "test_prob=re.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_prob,index=y_train.index,columns = ['prob', 'y'])\n",
    "train_df['y']=y_train\n",
    "test_df = pd.DataFrame(test_prob,index=y_test.index,columns = ['prob', 'y'])\n",
    "test_df['y']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.797752808988764"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=0\n",
    "cur=0\n",
    "num=int(0.05*len(train_df.index))\n",
    "for i in train_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=train_df.loc[i,'y']\n",
    "# train\n",
    "k/sum(train_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06493506493506493"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=0\n",
    "cur=0\n",
    "num=int(0.05*len(test_df.index))\n",
    "for i in test_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=test_df.loc[i,'y']\n",
    "# test\n",
    "k/sum(test_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
