{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import en_core_web_md\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV,LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "data = pd.read_csv(\"reddit_worldnews_start_to_2016-11-22.csv\", encoding='latin-1')\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>over_18</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>1.216517e+09</td>\n",
       "      <td>15.823906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>1.243837e+09</td>\n",
       "      <td>25.697511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1.278120e+09</td>\n",
       "      <td>38.427444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>1.308658e+09</td>\n",
       "      <td>53.979581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>1.342873e+09</td>\n",
       "      <td>92.048355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>1.373529e+09</td>\n",
       "      <td>91.272569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>1.404038e+09</td>\n",
       "      <td>119.458372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>1.436001e+09</td>\n",
       "      <td>161.502077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>1.465642e+09</td>\n",
       "      <td>182.005654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      time_created    up_votes  down_votes   over_18\n",
       "year                                                \n",
       "2008  1.216517e+09   15.823906         0.0  0.000267\n",
       "2009  1.243837e+09   25.697511         0.0  0.000438\n",
       "2010  1.278120e+09   38.427444         0.0  0.000257\n",
       "2011  1.308658e+09   53.979581         0.0  0.001543\n",
       "2012  1.342873e+09   92.048355         0.0  0.000516\n",
       "2013  1.373529e+09   91.272569         0.0  0.000918\n",
       "2014  1.404038e+09  119.458372         0.0  0.000456\n",
       "2015  1.436001e+09  161.502077         0.0  0.000528\n",
       "2016  1.465642e+09  182.005654         0.0  0.000513"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column to contain 'year'\n",
    "data['year']='2008'\n",
    "for i in data.index:\n",
    "    data.at[i,'year'] = data.at[i,'date_created'][:4]\n",
    "data.groupby(['year']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see that the up_votes were rising through time. The reason for this is that Internet and Reddit were becoming more and more popular during this period (2008-2016). To eliminate the influence of time series, we label the top 5% up_votes article of each year as \"hot article\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp is a dict that contains the 95% cutoff of each year\n",
    "temp={}\n",
    "for i in range(2008,2017):\n",
    "    temp[str(i)] = np.percentile(data[data['year']==str(i)]['up_votes'],95)\n",
    "\n",
    "# Mark hot articles\n",
    "data['label']=0\n",
    "for i in data.index:\n",
    "    if data.at[i,'up_votes'] >= temp[data.at[i,'year']]:\n",
    "        data.at[i,'label']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of hot articles:  25495\n",
      "# of non-hot articles:  483741\n"
     ]
    }
   ],
   "source": [
    "print('# of hot articles: ', len(data[data['label']==1]))\n",
    "print('# of non-hot articles: ', len(data[data['label']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the reviews (tokenizing, lemmatization, removing stopwords)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(titles):\n",
    "    filtered_titles = []\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        token_list = word_tokenize(title) # Tokenize\n",
    "        filtered_token = [t for t in token_list if not t in stop_words] # Remove stopwords\n",
    "        for i in range(len(filtered_token)):\n",
    "            filtered_token[i] = lemmatizer.lemmatize(filtered_token[i]).strip(string.punctuation) # Lemmatization\n",
    "        filtered_titles.append(\" \".join(filtered_token))\n",
    "    return filtered_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF vectorizer\n",
    "filtered_corpus = preprocessing(data[\"title\"])\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df = 0.4, max_features = 2000) # only use first 2000 features because of \n",
    "                                                                # computatioal complexity later on\n",
    "\n",
    "# vectorize the corpus\n",
    "vector = vectorizer.fit_transform(filtered_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbas</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abducted</th>\n",
       "      <th>abe</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abroad</th>\n",
       "      <th>abu</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>yemeni</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zika</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbas  abbott  abducted  abe  able  abortion  abroad  abu  abuse  accept  \\\n",
       "0    0.0     0.0       0.0  0.0   0.0       0.0     0.0  0.0    0.0     0.0   \n",
       "1    0.0     0.0       0.0  0.0   0.0       0.0     0.0  0.0    0.0     0.0   \n",
       "2    0.0     0.0       0.0  0.0   0.0       0.0     0.0  0.0    0.0     0.0   \n",
       "3    0.0     0.0       0.0  0.0   0.0       0.0     0.0  0.0    0.0     0.0   \n",
       "4    0.0     0.0       0.0  0.0   0.0       0.0     0.0  0.0    0.0     0.0   \n",
       "\n",
       "   ...   yemeni  yet  york  young  youth  youtube  zealand  zika  zimbabwe  \\\n",
       "0  ...      0.0  0.0   0.0    0.0    0.0      0.0      0.0   0.0       0.0   \n",
       "1  ...      0.0  0.0   0.0    0.0    0.0      0.0      0.0   0.0       0.0   \n",
       "2  ...      0.0  0.0   0.0    0.0    0.0      0.0      0.0   0.0       0.0   \n",
       "3  ...      0.0  0.0   0.0    0.0    0.0      0.0      0.0   0.0       0.0   \n",
       "4  ...      0.0  0.0   0.0    0.0    0.0      0.0      0.0   0.0       0.0   \n",
       "\n",
       "   zone  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF matrix\n",
    "tfidf_matrix = pd.DataFrame(vector.toarray(), columns = vectorizer.get_feature_names())\n",
    "tfidf_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word embeddings for each word in the column index of TF-IDF matrix\n",
    "word2vec = [np.array(nlp(i).vector) for i in tfidf_matrix.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.912060</td>\n",
       "      <td>0.486792</td>\n",
       "      <td>0.476928</td>\n",
       "      <td>0.289893</td>\n",
       "      <td>0.054770</td>\n",
       "      <td>-0.870862</td>\n",
       "      <td>-0.268372</td>\n",
       "      <td>0.243633</td>\n",
       "      <td>0.638600</td>\n",
       "      <td>3.758917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147540</td>\n",
       "      <td>-0.250236</td>\n",
       "      <td>-0.509609</td>\n",
       "      <td>0.120965</td>\n",
       "      <td>0.368058</td>\n",
       "      <td>0.258011</td>\n",
       "      <td>-0.647812</td>\n",
       "      <td>-0.213497</td>\n",
       "      <td>0.221647</td>\n",
       "      <td>0.039148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.324209</td>\n",
       "      <td>-0.390879</td>\n",
       "      <td>-0.141281</td>\n",
       "      <td>0.468408</td>\n",
       "      <td>0.106328</td>\n",
       "      <td>0.084631</td>\n",
       "      <td>-0.485883</td>\n",
       "      <td>0.411318</td>\n",
       "      <td>0.092061</td>\n",
       "      <td>3.285786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568110</td>\n",
       "      <td>0.480141</td>\n",
       "      <td>0.255341</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>-0.604799</td>\n",
       "      <td>-0.348221</td>\n",
       "      <td>0.065231</td>\n",
       "      <td>-0.195351</td>\n",
       "      <td>-0.358770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.392182</td>\n",
       "      <td>0.212705</td>\n",
       "      <td>0.292572</td>\n",
       "      <td>-0.265595</td>\n",
       "      <td>0.723275</td>\n",
       "      <td>-0.399778</td>\n",
       "      <td>-0.584884</td>\n",
       "      <td>0.070501</td>\n",
       "      <td>-0.410889</td>\n",
       "      <td>2.268124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122901</td>\n",
       "      <td>0.364871</td>\n",
       "      <td>-0.286764</td>\n",
       "      <td>-0.455900</td>\n",
       "      <td>0.556184</td>\n",
       "      <td>-0.654148</td>\n",
       "      <td>-0.201398</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>-0.320886</td>\n",
       "      <td>0.658890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.495490</td>\n",
       "      <td>0.646893</td>\n",
       "      <td>-0.114675</td>\n",
       "      <td>-0.209844</td>\n",
       "      <td>-0.494770</td>\n",
       "      <td>-0.815250</td>\n",
       "      <td>-0.022099</td>\n",
       "      <td>-0.126436</td>\n",
       "      <td>0.217735</td>\n",
       "      <td>6.272498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.395701</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.125113</td>\n",
       "      <td>-0.427662</td>\n",
       "      <td>0.136894</td>\n",
       "      <td>-0.184926</td>\n",
       "      <td>-0.489197</td>\n",
       "      <td>-0.181203</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>0.072146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.521825</td>\n",
       "      <td>0.040879</td>\n",
       "      <td>0.591420</td>\n",
       "      <td>0.060628</td>\n",
       "      <td>0.589623</td>\n",
       "      <td>0.026628</td>\n",
       "      <td>-0.254125</td>\n",
       "      <td>0.444144</td>\n",
       "      <td>-0.203941</td>\n",
       "      <td>4.084317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188162</td>\n",
       "      <td>0.022798</td>\n",
       "      <td>0.389346</td>\n",
       "      <td>-0.072245</td>\n",
       "      <td>-0.124132</td>\n",
       "      <td>0.184864</td>\n",
       "      <td>0.015786</td>\n",
       "      <td>0.147740</td>\n",
       "      <td>-0.223388</td>\n",
       "      <td>0.561387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.912060  0.486792  0.476928  0.289893  0.054770 -0.870862 -0.268372   \n",
       "1 -0.324209 -0.390879 -0.141281  0.468408  0.106328  0.084631 -0.485883   \n",
       "2 -0.392182  0.212705  0.292572 -0.265595  0.723275 -0.399778 -0.584884   \n",
       "3 -0.495490  0.646893 -0.114675 -0.209844 -0.494770 -0.815250 -0.022099   \n",
       "4 -0.521825  0.040879  0.591420  0.060628  0.589623  0.026628 -0.254125   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.243633  0.638600  3.758917    ...     0.147540 -0.250236 -0.509609   \n",
       "1  0.411318  0.092061  3.285786    ...     0.568110  0.480141  0.255341   \n",
       "2  0.070501 -0.410889  2.268124    ...     0.122901  0.364871 -0.286764   \n",
       "3 -0.126436  0.217735  6.272498    ...    -0.395701  0.316228 -0.125113   \n",
       "4  0.444144 -0.203941  4.084317    ...    -0.188162  0.022798  0.389346   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.120965  0.368058  0.258011 -0.647812 -0.213497  0.221647  0.039148  \n",
       "1  0.012618 -0.005795 -0.604799 -0.348221  0.065231 -0.195351 -0.358770  \n",
       "2 -0.455900  0.556184 -0.654148 -0.201398  0.354193 -0.320886  0.658890  \n",
       "3 -0.427662  0.136894 -0.184926 -0.489197 -0.181203  0.021290  0.072146  \n",
       "4 -0.072245 -0.124132  0.184864  0.015786  0.147740 -0.223388  0.561387  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each title, use each word's TF-IDF mutliply by its word embeddings vector and sum all the word vectors\n",
    "# The result is an unweighted matrix for each title\n",
    "unweighted_matrix = pd.DataFrame(np.dot(tfidf_matrix,np.array(word2vec)))\n",
    "unweighted_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.465222</td>\n",
       "      <td>0.248302</td>\n",
       "      <td>0.243271</td>\n",
       "      <td>0.147868</td>\n",
       "      <td>0.027937</td>\n",
       "      <td>-0.444208</td>\n",
       "      <td>-0.136891</td>\n",
       "      <td>0.124272</td>\n",
       "      <td>0.325736</td>\n",
       "      <td>1.917342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>-0.127640</td>\n",
       "      <td>-0.259941</td>\n",
       "      <td>0.061702</td>\n",
       "      <td>0.187738</td>\n",
       "      <td>0.131606</td>\n",
       "      <td>-0.330435</td>\n",
       "      <td>-0.108900</td>\n",
       "      <td>0.113058</td>\n",
       "      <td>0.019968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.189500</td>\n",
       "      <td>-0.228469</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>0.273784</td>\n",
       "      <td>0.062149</td>\n",
       "      <td>0.049467</td>\n",
       "      <td>-0.283999</td>\n",
       "      <td>0.240415</td>\n",
       "      <td>0.053810</td>\n",
       "      <td>1.920541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332060</td>\n",
       "      <td>0.280642</td>\n",
       "      <td>0.149247</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>-0.003387</td>\n",
       "      <td>-0.353505</td>\n",
       "      <td>-0.203535</td>\n",
       "      <td>0.038127</td>\n",
       "      <td>-0.114182</td>\n",
       "      <td>-0.209701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.197132</td>\n",
       "      <td>0.106917</td>\n",
       "      <td>0.147062</td>\n",
       "      <td>-0.133502</td>\n",
       "      <td>0.363557</td>\n",
       "      <td>-0.200950</td>\n",
       "      <td>-0.293994</td>\n",
       "      <td>0.035438</td>\n",
       "      <td>-0.206535</td>\n",
       "      <td>1.140080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061777</td>\n",
       "      <td>0.183404</td>\n",
       "      <td>-0.144143</td>\n",
       "      <td>-0.229160</td>\n",
       "      <td>0.279568</td>\n",
       "      <td>-0.328809</td>\n",
       "      <td>-0.101234</td>\n",
       "      <td>0.178036</td>\n",
       "      <td>-0.161294</td>\n",
       "      <td>0.331193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.222323</td>\n",
       "      <td>0.290256</td>\n",
       "      <td>-0.051454</td>\n",
       "      <td>-0.094156</td>\n",
       "      <td>-0.222000</td>\n",
       "      <td>-0.365797</td>\n",
       "      <td>-0.009915</td>\n",
       "      <td>-0.056731</td>\n",
       "      <td>0.097696</td>\n",
       "      <td>2.814425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177548</td>\n",
       "      <td>0.141889</td>\n",
       "      <td>-0.056137</td>\n",
       "      <td>-0.191889</td>\n",
       "      <td>0.061423</td>\n",
       "      <td>-0.082975</td>\n",
       "      <td>-0.219499</td>\n",
       "      <td>-0.081305</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>0.032371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.301983</td>\n",
       "      <td>0.023657</td>\n",
       "      <td>0.342258</td>\n",
       "      <td>0.035086</td>\n",
       "      <td>0.341218</td>\n",
       "      <td>0.015410</td>\n",
       "      <td>-0.147064</td>\n",
       "      <td>0.257029</td>\n",
       "      <td>-0.118022</td>\n",
       "      <td>2.363621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108891</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>0.225317</td>\n",
       "      <td>-0.041809</td>\n",
       "      <td>-0.071836</td>\n",
       "      <td>0.106982</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.085498</td>\n",
       "      <td>-0.129276</td>\n",
       "      <td>0.324879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.465222  0.248302  0.243271  0.147868  0.027937 -0.444208 -0.136891   \n",
       "1 -0.189500 -0.228469 -0.082579  0.273784  0.062149  0.049467 -0.283999   \n",
       "2 -0.197132  0.106917  0.147062 -0.133502  0.363557 -0.200950 -0.293994   \n",
       "3 -0.222323  0.290256 -0.051454 -0.094156 -0.222000 -0.365797 -0.009915   \n",
       "4 -0.301983  0.023657  0.342258  0.035086  0.341218  0.015410 -0.147064   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.124272  0.325736  1.917342    ...     0.075257 -0.127640 -0.259941   \n",
       "1  0.240415  0.053810  1.920541    ...     0.332060  0.280642  0.149247   \n",
       "2  0.035438 -0.206535  1.140080    ...     0.061777  0.183404 -0.144143   \n",
       "3 -0.056731  0.097696  2.814425    ...    -0.177548  0.141889 -0.056137   \n",
       "4  0.257029 -0.118022  2.363621    ...    -0.108891  0.013193  0.225317   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.061702  0.187738  0.131606 -0.330435 -0.108900  0.113058  0.019968  \n",
       "1  0.007375 -0.003387 -0.353505 -0.203535  0.038127 -0.114182 -0.209701  \n",
       "2 -0.229160  0.279568 -0.328809 -0.101234  0.178036 -0.161294  0.331193  \n",
       "3 -0.191889  0.061423 -0.082975 -0.219499 -0.081305  0.009553  0.032371  \n",
       "4 -0.041809 -0.071836  0.106982  0.009136  0.085498 -0.129276  0.324879  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each title, use unweighted matrix divided by the sum of that title's TF-IDF to get weighted word2vec matrix\n",
    "# The result is our final word2vec matrix\n",
    "final_w2v = unweighted_matrix.div(tfidf_matrix.sum(axis=1), axis=0)\n",
    "final_w2v = final_w2v.fillna(0)\n",
    "final_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509236, 301)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add label to final_w2v\n",
    "final_w2v[\"label\"] = np.array(data[\"label\"])\n",
    "final_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random split train(70%) and test(30%)\n",
    "reddit_train, reddit_test, y_train, y_test = train_test_split(\n",
    "    final_w2v.iloc[:,:-1],\n",
    "    final_w2v[\"label\"],\n",
    "    test_size=0.3,\n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### author = 1 if he/she had writen a hot article (in train dataset)\n",
    "author=set()\n",
    "for i in final_w2v.index:\n",
    "    if final_w2v.at[i,'label']==1:\n",
    "        author.add(data.at[i,'author'])\n",
    "        \n",
    "reddit_train['hot_author']=0\n",
    "reddit_test['hot_author']=0\n",
    "for i in reddit_train.index:\n",
    "    if data.at[i,'author'] in author:\n",
    "        reddit_train.at[i,'hot_author']=1\n",
    "for i in reddit_test.index:\n",
    "    if data.at[i,'author'] in author:\n",
    "        reddit_test.at[i,'hot_author']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append w2v and author variable together\n",
    "final_variables = pd.DataFrame(np.concatenate((reddit_train,reddit_test)), columns = reddit_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509236, 141)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep 90% of original information\n",
    "pca = PCA(n_components = 0.9)\n",
    "pca_features = pca.fit_transform(np.array(final_variables))\n",
    "pca_df = pd.DataFrame(pca_features)\n",
    "pca_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get train and test independent variables\n",
    "train_row = len(reddit_train)\n",
    "test_row = len(reddit_test)\n",
    "X_train = pca_df[:int(train_row)]\n",
    "X_test = pca_df[-int(test_row):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegressionCV(multi_class=\"ovr\",fit_intercept=True,Cs=np.logspace(-2,2,20),cv=2,penalty=\"l2\",solver=\"lbfgs\",tol=0.01)\n",
    "re = lr.fit(X_train,y_train)\n",
    "train_prob=re.predict_proba(X_train)\n",
    "test_prob=re.predict_proba(X_test)\n",
    "train_df = pd.DataFrame(train_prob,index=y_train.index,columns = ['prob', 'y'])\n",
    "train_df['y']=y_train\n",
    "test_df = pd.DataFrame(test_prob,index=y_test.index,columns = ['prob', 'y'])\n",
    "test_df['y']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train FDR@5% of Logistic regression: 17.02%\n"
     ]
    }
   ],
   "source": [
    "# Get the train FDR@5% of Logistic regression\n",
    "k=0\n",
    "cur=0\n",
    "num=int(0.05*len(train_df.index))\n",
    "for i in train_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=train_df.loc[i,'y']\n",
    "# train\n",
    "print('train FDR@5% of Logistic regression:', '{:.2%}'.format(k/sum(train_df['y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test FDR@5% of Logistic regression: 17.14%\n"
     ]
    }
   ],
   "source": [
    "# Get the test FDR@5% of Logistic regression\n",
    "k=0\n",
    "cur=0\n",
    "num=int(0.05*len(test_df.index))\n",
    "for i in test_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=test_df.loc[i,'y']\n",
    "# test\n",
    "print('test FDR@5% of Logistic regression:', '{:.2%}'.format(k/sum(test_df['y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBDT\n",
    "gbm = GradientBoostingClassifier(random_state=10)\n",
    "gbm.fit(X_train,y_train)\n",
    "train_prob=gbm.predict_proba(X_train)\n",
    "test_prob=gbm.predict_proba(X_test)\n",
    "train_df = pd.DataFrame(train_prob,index=y_train.index,columns = ['prob', 'y'])\n",
    "train_df['y']=y_train\n",
    "test_df = pd.DataFrame(test_prob,index=y_test.index,columns = ['prob', 'y'])\n",
    "test_df['y']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train FDR@5% of GBDT: 20.79%\n"
     ]
    }
   ],
   "source": [
    "# Get the train FDR@5% of Gradient Boost Decision Tree\n",
    "k=0\n",
    "cur=0\n",
    "num=int(0.05*len(train_df.index))\n",
    "for i in train_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=train_df.loc[i,'y']\n",
    "# train\n",
    "print('train FDR@5% of GBDT:', '{:.2%}'.format(k/sum(train_df['y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test FDR@5% of GBDT: 18.40%\n"
     ]
    }
   ],
   "source": [
    "# Get the test FDR@5% of Gradient Boost Decision Tree\n",
    "k=0\n",
    "cur=0\n",
    "num=int(0.05*len(test_df.index))\n",
    "for i in test_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=test_df.loc[i,'y']\n",
    "# test\n",
    "print('test FDR@5% of GBDT:', '{:.2%}'.format(k/sum(test_df['y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(21,6), random_state=1)\n",
    "nn.fit(X_train, y_train)\n",
    "train_prob=nn.predict_proba(X_train)\n",
    "test_prob=nn.predict_proba(X_test)\n",
    "train_df = pd.DataFrame(train_prob,index=y_train.index,columns = ['prob', 'y'])\n",
    "train_df['y']=y_train\n",
    "test_df = pd.DataFrame(test_prob,index=y_test.index,columns = ['prob', 'y'])\n",
    "test_df['y']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train FDR@5% of Neural Network: 28.31%\n"
     ]
    }
   ],
   "source": [
    "# Get the train FDR@5% of Neural Network\n",
    "k=0\n",
    "cur=0\n",
    "num=int(0.05*len(train_df.index))\n",
    "for i in train_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=train_df.loc[i,'y']\n",
    "# train\n",
    "print('train FDR@5% of Neural Network:', '{:.2%}'.format(k/sum(train_df['y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test FDR@5% of Neural Network: 19.86%\n"
     ]
    }
   ],
   "source": [
    "# Get the test FDR@5% of Neural Network\n",
    "k=0\n",
    "cur=0\n",
    "num=int(0.05*len(test_df.index))\n",
    "for i in test_df.sort_values(by='prob').index:\n",
    "    if cur<num:\n",
    "        cur+=1\n",
    "        k+=test_df.loc[i,'y']\n",
    "# test\n",
    "print('test FDR@5% of Neural Network:', '{:.2%}'.format(k/sum(test_df['y'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
