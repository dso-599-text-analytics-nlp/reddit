{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"reddit_worldnews_start_to_2016-11-22.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are labeling the post as popular or not according to the number of up votes.\n",
    "\n",
    "Since we see an increasing number of up votes over year, we add \"popular\" label based on whether the up votes are in the top 5% percentile in every year's posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year']='2008'\n",
    "for i in data.index:\n",
    "    data.at[i,'year'] = data.at[i,'date_created'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp is a dict that contains the 95% cutoff of each year\n",
    "temp={}\n",
    "for i in range(2008,2017):\n",
    "    temp[str(i)] = np.percentile(data[data['year']==str(i)]['up_votes'],95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label']=0\n",
    "for i in data.index:\n",
    "    if data.at[i,'up_votes'] >= temp[data.at[i,'year']]:\n",
    "        data.at[i,'label']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in spacy\n",
    "import en_core_web_md\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "# Preprocess the reviews (tokenizing, lemmatization, removing stopwords)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(titles):\n",
    "    filtered_titles = []\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        token_list = word_tokenize(title) # Tokenize\n",
    "        filtered_token = [t for t in token_list if not t in stop_words] # Remove stopwords\n",
    "        for i in range(len(filtered_token)):\n",
    "            filtered_token[i] = lemmatizer.lemmatize(filtered_token[i]).strip(string.punctuation) # Lemmatization\n",
    "        filtered_titles.append(\" \".join(filtered_token))\n",
    "    return filtered_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF weighted word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorizer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# data\n",
    "filtered_corpus = preprocessing(data[\"title\"])\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df = 0.4, max_features = 2000) # only use first 2000 features because of \n",
    "                                                                # computatioal complexity later on\n",
    "\n",
    "# vectorize the corpus\n",
    "vector = vectorizer.fit_transform(filtered_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.912060</td>\n",
       "      <td>0.486792</td>\n",
       "      <td>0.476928</td>\n",
       "      <td>0.289893</td>\n",
       "      <td>0.054770</td>\n",
       "      <td>-0.870862</td>\n",
       "      <td>-0.268372</td>\n",
       "      <td>0.243633</td>\n",
       "      <td>0.638600</td>\n",
       "      <td>3.758917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147540</td>\n",
       "      <td>-0.250236</td>\n",
       "      <td>-0.509609</td>\n",
       "      <td>0.120965</td>\n",
       "      <td>0.368058</td>\n",
       "      <td>0.258011</td>\n",
       "      <td>-0.647812</td>\n",
       "      <td>-0.213497</td>\n",
       "      <td>0.221647</td>\n",
       "      <td>0.039148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.324209</td>\n",
       "      <td>-0.390879</td>\n",
       "      <td>-0.141281</td>\n",
       "      <td>0.468408</td>\n",
       "      <td>0.106328</td>\n",
       "      <td>0.084631</td>\n",
       "      <td>-0.485883</td>\n",
       "      <td>0.411318</td>\n",
       "      <td>0.092061</td>\n",
       "      <td>3.285786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568110</td>\n",
       "      <td>0.480141</td>\n",
       "      <td>0.255341</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>-0.604799</td>\n",
       "      <td>-0.348221</td>\n",
       "      <td>0.065231</td>\n",
       "      <td>-0.195351</td>\n",
       "      <td>-0.358770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.392182</td>\n",
       "      <td>0.212705</td>\n",
       "      <td>0.292572</td>\n",
       "      <td>-0.265595</td>\n",
       "      <td>0.723275</td>\n",
       "      <td>-0.399778</td>\n",
       "      <td>-0.584884</td>\n",
       "      <td>0.070501</td>\n",
       "      <td>-0.410889</td>\n",
       "      <td>2.268124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122901</td>\n",
       "      <td>0.364871</td>\n",
       "      <td>-0.286764</td>\n",
       "      <td>-0.455900</td>\n",
       "      <td>0.556184</td>\n",
       "      <td>-0.654148</td>\n",
       "      <td>-0.201398</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>-0.320886</td>\n",
       "      <td>0.658890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.495490</td>\n",
       "      <td>0.646893</td>\n",
       "      <td>-0.114675</td>\n",
       "      <td>-0.209844</td>\n",
       "      <td>-0.494770</td>\n",
       "      <td>-0.815250</td>\n",
       "      <td>-0.022099</td>\n",
       "      <td>-0.126436</td>\n",
       "      <td>0.217735</td>\n",
       "      <td>6.272498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.395701</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.125113</td>\n",
       "      <td>-0.427662</td>\n",
       "      <td>0.136894</td>\n",
       "      <td>-0.184926</td>\n",
       "      <td>-0.489197</td>\n",
       "      <td>-0.181203</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>0.072146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.521825</td>\n",
       "      <td>0.040879</td>\n",
       "      <td>0.591420</td>\n",
       "      <td>0.060628</td>\n",
       "      <td>0.589623</td>\n",
       "      <td>0.026628</td>\n",
       "      <td>-0.254125</td>\n",
       "      <td>0.444144</td>\n",
       "      <td>-0.203941</td>\n",
       "      <td>4.084317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188162</td>\n",
       "      <td>0.022798</td>\n",
       "      <td>0.389346</td>\n",
       "      <td>-0.072245</td>\n",
       "      <td>-0.124132</td>\n",
       "      <td>0.184864</td>\n",
       "      <td>0.015786</td>\n",
       "      <td>0.147740</td>\n",
       "      <td>-0.223388</td>\n",
       "      <td>0.561387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.912060  0.486792  0.476928  0.289893  0.054770 -0.870862 -0.268372   \n",
       "1 -0.324209 -0.390879 -0.141281  0.468408  0.106328  0.084631 -0.485883   \n",
       "2 -0.392182  0.212705  0.292572 -0.265595  0.723275 -0.399778 -0.584884   \n",
       "3 -0.495490  0.646893 -0.114675 -0.209844 -0.494770 -0.815250 -0.022099   \n",
       "4 -0.521825  0.040879  0.591420  0.060628  0.589623  0.026628 -0.254125   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.243633  0.638600  3.758917    ...     0.147540 -0.250236 -0.509609   \n",
       "1  0.411318  0.092061  3.285786    ...     0.568110  0.480141  0.255341   \n",
       "2  0.070501 -0.410889  2.268124    ...     0.122901  0.364871 -0.286764   \n",
       "3 -0.126436  0.217735  6.272498    ...    -0.395701  0.316228 -0.125113   \n",
       "4  0.444144 -0.203941  4.084317    ...    -0.188162  0.022798  0.389346   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.120965  0.368058  0.258011 -0.647812 -0.213497  0.221647  0.039148  \n",
       "1  0.012618 -0.005795 -0.604799 -0.348221  0.065231 -0.195351 -0.358770  \n",
       "2 -0.455900  0.556184 -0.654148 -0.201398  0.354193 -0.320886  0.658890  \n",
       "3 -0.427662  0.136894 -0.184926 -0.489197 -0.181203  0.021290  0.072146  \n",
       "4 -0.072245 -0.124132  0.184864  0.015786  0.147740 -0.223388  0.561387  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF matrix\n",
    "tfidf_matrix = pd.DataFrame(vector.toarray(), columns = vectorizer.get_feature_names())\n",
    "# Word embeddings for each word in the column index of TF-IDF matrix\n",
    "word2vec = [np.array(nlp(i).vector) for i in tfidf_matrix.columns]\n",
    "# For each title, use each word's TF-IDF mutliply by its word embeddings vector and sum all the word vectors\n",
    "# The result is an unweighted matrix for each title\n",
    "unweighted_matrix = pd.DataFrame(np.dot(tfidf_matrix,np.array(word2vec)))\n",
    "unweighted_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.465222</td>\n",
       "      <td>0.248302</td>\n",
       "      <td>0.243271</td>\n",
       "      <td>0.147868</td>\n",
       "      <td>0.027937</td>\n",
       "      <td>-0.444208</td>\n",
       "      <td>-0.136891</td>\n",
       "      <td>0.124272</td>\n",
       "      <td>0.325736</td>\n",
       "      <td>1.917342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>-0.127640</td>\n",
       "      <td>-0.259941</td>\n",
       "      <td>0.061702</td>\n",
       "      <td>0.187738</td>\n",
       "      <td>0.131606</td>\n",
       "      <td>-0.330435</td>\n",
       "      <td>-0.108900</td>\n",
       "      <td>0.113058</td>\n",
       "      <td>0.019968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.189500</td>\n",
       "      <td>-0.228469</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>0.273784</td>\n",
       "      <td>0.062149</td>\n",
       "      <td>0.049467</td>\n",
       "      <td>-0.283999</td>\n",
       "      <td>0.240415</td>\n",
       "      <td>0.053810</td>\n",
       "      <td>1.920541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332060</td>\n",
       "      <td>0.280642</td>\n",
       "      <td>0.149247</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>-0.003387</td>\n",
       "      <td>-0.353505</td>\n",
       "      <td>-0.203535</td>\n",
       "      <td>0.038127</td>\n",
       "      <td>-0.114182</td>\n",
       "      <td>-0.209701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.197132</td>\n",
       "      <td>0.106917</td>\n",
       "      <td>0.147062</td>\n",
       "      <td>-0.133502</td>\n",
       "      <td>0.363557</td>\n",
       "      <td>-0.200950</td>\n",
       "      <td>-0.293994</td>\n",
       "      <td>0.035438</td>\n",
       "      <td>-0.206535</td>\n",
       "      <td>1.140080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061777</td>\n",
       "      <td>0.183404</td>\n",
       "      <td>-0.144143</td>\n",
       "      <td>-0.229160</td>\n",
       "      <td>0.279568</td>\n",
       "      <td>-0.328809</td>\n",
       "      <td>-0.101234</td>\n",
       "      <td>0.178036</td>\n",
       "      <td>-0.161294</td>\n",
       "      <td>0.331193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.222323</td>\n",
       "      <td>0.290256</td>\n",
       "      <td>-0.051454</td>\n",
       "      <td>-0.094156</td>\n",
       "      <td>-0.222000</td>\n",
       "      <td>-0.365797</td>\n",
       "      <td>-0.009915</td>\n",
       "      <td>-0.056731</td>\n",
       "      <td>0.097696</td>\n",
       "      <td>2.814425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177548</td>\n",
       "      <td>0.141889</td>\n",
       "      <td>-0.056137</td>\n",
       "      <td>-0.191889</td>\n",
       "      <td>0.061423</td>\n",
       "      <td>-0.082975</td>\n",
       "      <td>-0.219499</td>\n",
       "      <td>-0.081305</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>0.032371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.301983</td>\n",
       "      <td>0.023657</td>\n",
       "      <td>0.342258</td>\n",
       "      <td>0.035086</td>\n",
       "      <td>0.341218</td>\n",
       "      <td>0.015410</td>\n",
       "      <td>-0.147064</td>\n",
       "      <td>0.257029</td>\n",
       "      <td>-0.118022</td>\n",
       "      <td>2.363621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108891</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>0.225317</td>\n",
       "      <td>-0.041809</td>\n",
       "      <td>-0.071836</td>\n",
       "      <td>0.106982</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.085498</td>\n",
       "      <td>-0.129276</td>\n",
       "      <td>0.324879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.465222  0.248302  0.243271  0.147868  0.027937 -0.444208 -0.136891   \n",
       "1 -0.189500 -0.228469 -0.082579  0.273784  0.062149  0.049467 -0.283999   \n",
       "2 -0.197132  0.106917  0.147062 -0.133502  0.363557 -0.200950 -0.293994   \n",
       "3 -0.222323  0.290256 -0.051454 -0.094156 -0.222000 -0.365797 -0.009915   \n",
       "4 -0.301983  0.023657  0.342258  0.035086  0.341218  0.015410 -0.147064   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.124272  0.325736  1.917342    ...     0.075257 -0.127640 -0.259941   \n",
       "1  0.240415  0.053810  1.920541    ...     0.332060  0.280642  0.149247   \n",
       "2  0.035438 -0.206535  1.140080    ...     0.061777  0.183404 -0.144143   \n",
       "3 -0.056731  0.097696  2.814425    ...    -0.177548  0.141889 -0.056137   \n",
       "4  0.257029 -0.118022  2.363621    ...    -0.108891  0.013193  0.225317   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.061702  0.187738  0.131606 -0.330435 -0.108900  0.113058  0.019968  \n",
       "1  0.007375 -0.003387 -0.353505 -0.203535  0.038127 -0.114182 -0.209701  \n",
       "2 -0.229160  0.279568 -0.328809 -0.101234  0.178036 -0.161294  0.331193  \n",
       "3 -0.191889  0.061423 -0.082975 -0.219499 -0.081305  0.009553  0.032371  \n",
       "4 -0.041809 -0.071836  0.106982  0.009136  0.085498 -0.129276  0.324879  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each title, use unweighted matrix divided by the sum of that title's TF-IDF to get weighted word2vec matrix\n",
    "# The result is our final word2vec matrix\n",
    "final_w2v = unweighted_matrix.div(tfidf_matrix.sum(axis=1), axis=0)\n",
    "final_w2v = final_w2v.fillna(0)\n",
    "final_w2v.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA to reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# keep 80% of original information\n",
    "pca = PCA(n_components = 0.8)\n",
    "pca_features = pca.fit_transform(np.array(final_w2v))\n",
    "pca_df = pd.DataFrame(pca_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pca_df,\n",
    "    data.iloc[:,-1],\n",
    "    test_size=0.3,\n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356465, 88)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.05\n",
      "Detection Rate at 5% (testing): 0.123\n",
      "Learning rate:  0.1\n",
      "Detection Rate at 5% (testing): 0.126\n",
      "Learning rate:  0.25\n",
      "Detection Rate at 5% (testing): 0.134\n",
      "Learning rate:  0.5\n",
      "Detection Rate at 5% (testing): 0.135\n",
      "Learning rate:  0.75\n",
      "Detection Rate at 5% (testing): 0.130\n",
      "Learning rate:  1\n",
      "Detection Rate at 5% (testing): 0.130\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for learning_rate in learning_rates:\n",
    "    gb = GradientBoostingClassifier(n_estimators=X_train.shape[1], learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n",
    "    gb.fit(X_train, y_train)\n",
    "    \n",
    "    train_prob=gb.predict_proba(X_train)\n",
    "    test_prob=gb.predict_proba(X_test)\n",
    "    train_df = pd.DataFrame(train_prob,index=y_train.index,columns = ['prob', 'y'])\n",
    "    train_df['y']=y_train\n",
    "    test_df = pd.DataFrame(test_prob,index=y_test.index,columns = ['prob', 'y'])\n",
    "    test_df['y']=y_test\n",
    "    \n",
    "    #k=0\n",
    "    #cur=0\n",
    "    #num=int(0.05*len(train_df.index))\n",
    "    #for i in train_df.sort_values(by='prob').index:\n",
    "        #if cur<num:\n",
    "            #cur+=1\n",
    "            #k+=train_df.loc[i,'y']\n",
    "    # train\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    #print(\"Detection Rate at 5% (training): {0:.3f}\".format(k/sum(train_df['y']))\n",
    "          \n",
    "    p=0\n",
    "    cur=0\n",
    "    num=int(0.05*len(test_df.index))\n",
    "    for i in test_df.sort_values(by='prob').index:\n",
    "        if cur<num:\n",
    "            cur+=1\n",
    "            p+=test_df.loc[i,'y']\n",
    "    # test\n",
    "    rate=p/sum(test_df['y'])\n",
    "    print(\"Detection Rate at 5% (testing): {0:.3f}\".format(rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best learning rate is 0.5 when looking at testing detection rate at 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neighbors = np.arange(1,9)\n",
    "train_accuracy =np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    train_prob=knn.predict_proba(X_train)\n",
    "    test_prob=knn.predict_proba(X_test)\n",
    "    train_df = pd.DataFrame(train_prob,index=y_train.index,columns = ['prob', 'y'])\n",
    "    train_df['y']=y_train\n",
    "    test_df = pd.DataFrame(test_prob,index=y_test.index,columns = ['prob', 'y'])\n",
    "    test_df['y']=y_test\n",
    "    \n",
    "    p=0\n",
    "    cur=0\n",
    "    num=int(0.05*len(test_df.index))\n",
    "    for i in test_df.sort_values(by='prob').index:\n",
    "        if cur<num:\n",
    "            cur+=1\n",
    "            p+=test_df.loc[i,'y']\n",
    "    # test\n",
    "    rate=p/sum(test_df['y'])\n",
    "    print(\"Number of Neighbors: \", k)\n",
    "    print(\"Detection Rate at 5% (testing): {0:.3f}\".format(rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "#unit = number of neurons in the layer\n",
    "clf1 = Sequential([\n",
    "    Dense(units=9, kernel_initializer='normal', input_dim=howManySelectedFeaturesPutIntoModel, activation='relu'),\n",
    "    Dense(units=14, kernel_initializer='normal', activation='softmax'),\n",
    "    Dropout(0.25),\n",
    "    Dense(1, kernel_initializer='normal', activation='sigmoid')\n",
    "])\n",
    "clf1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
